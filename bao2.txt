// Importing necessary Spark libraries
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD

// Set log level to ERROR to reduce log output
spark.sparkContext.setLogLevel("ERROR")

// Create an RDD from a list of tuples
val inputRDD = spark.sparkContext.parallelize(List(("Z", 1), ("A", 20), ("B", 30), ("C", 40), ("B", 30), ("B", 60)))

// Create another RDD from a list of integers
val listRdd = spark.sparkContext.parallelize(List(1, 2, 3, 4, 5, 3, 2))

// Define aggregation functions
def param0 = (accu: Int, v: Int) => accu + v
def param1 = (accu1: Int, accu2: Int) => accu1 + accu2

// Aggregate example with listRdd
println("aggregate : " + listRdd.aggregate(0)(param0, param1))
// Expected Output: aggregate : 20

// Define another set of aggregation functions
def param3 = (accu: Int, v: (String, Int)) => accu + v._2
def param4 = (accu1: Int, accu2: Int) => accu1 + accu2

// Aggregate example with inputRDD
println("aggregate : " + inputRDD.aggregate(0)(param3, param4))
